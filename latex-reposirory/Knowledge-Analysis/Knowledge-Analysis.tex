\documentclass[11pt]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{subfigure}
%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Query Expansion through Knowledge to Support Clinical Decisions}

\author{Hanwei Zhang, \, Junfeng Tian\\
  Department of Computer Science and Technology \\
  East China Normal University \\
  {\tt 52151201001, 51151201048@ecnu.cn} \\
  }
\date{}

\begin{document}
\bibliographystyle{acl}

\maketitle
\begin{abstract}
%Question? Current Problem? Our Method? Why? Result?
%IR,
%query document gap
%Knowledge Expansion
%Result
In this paper we present our systems for 2015 TREC Clinical Decision Support track (CDS). The goal is to retrieve relevant biomedical articles for answering generic clinical questions about medical records. We emphasized question how to retrieve relevant obscure articles from the query which described based on the words we all know, like symptoms. There always a gap between the query and document. We proposed knowledge-based query expansion to support clinical decisions, with two piloted methods to build a bridge connect common knowledge we all know(query) and domain-related knowledge(document), from common knowledge aspect and from domain knowledge aspect.
\end{abstract}

\section{Introduction}

%background
In making clinical decisions, physicians often seek out information about how to best care for their patients. To make biomedical information more accessible and to meet the requirements for the meaningful use of electronic health records.\\
\\
%related work
The Clinical Decision Support Track \cite{simpson2014overview} aims to simulate the requirements of such systems and to encourage the creation of tools and resources necessary for their implementation.\\
\\
%our work
In this report, we present our systems for the track. We proposed knowledge-based query expansion to support clinical decisions, with two piloted methods to build a bridge connect common knowledge we all know(query) and domain-related knowledge(document), from common knowledge aspect and from domain knowledge aspect.\\
%section overview
\\
This report is structured as follows. Section~\ref{sec:outsys} introduces our systems including basic Information Retrieval model and our proposed two method of knowledge-base query expansion. Experiment for our approach is described in Section~\ref{sec:expr}. And Results from all of our runs are discussed in Section~\ref{sec:results} and our findings are summarized in Section~\ref{sec:conclusion}.

\section{Our Systems}
\label{sec:outsys}
%section overview

In this section, we will introduce our knowledge-based query expansion model to link the query and document. Firstly, we will discuss the Information Retrieval System we use, A Smoothed Polya Urn Document Model(SPUD). Then we present our two query expansion method using External Knowledge to improve our system, from Common Knowledge and Domain Knowledge respectively.


\subsection{Smoothed Polya Urn Document Model}

We follow the work of \cite{DBLP:journals/corr/CumminsPL15} which contains three components, (i) Document Models, (ii) Query Models, (iii) Psuedo-Relevance Feedback Models. Here we briefly introduce the Models, refer to \cite{DBLP:journals/corr/CumminsPL15} for more details.

\begin{itemize}
\item {\bf Document Models:} We model each document as a mixture of multivariate Polya distributions. Based on the dependencies between recurrences of the same word-type, we combine topic model and background model respectively with hyper-parameter $\omega$ controls the smoothing.\\
\item {\bf Query Models:} Observing that some terms may be generated by our prior knowledge, we assume that long natural language queries are generated by drawing terms not only from a topical language model, but also a background query language model, with hyper-parameter $\omega$ controls the balance.\\
\item {\bf Psuedo-Relevance Feedback Models:} Firstly, we use the standard RM3 model \cite{lavrenko2001relevance} with the SPUD framework, where we assume the top documents are relevant. Secondly, we use an approach that extracts the topical terms from the feedback documents in a similar manner to that outlined.
\end{itemize}
We already have constructed our based Information Retrieval(IR) system, next we will introduce our two Query Expansion method using common knowledge and domain-related knowledge respectively.


\subsection{Expanded Words Extraction}

Since symptom is described based on the words we all know, while information relevant in published biomedical literature is obscure and difficult for us except physicians to understand. There always a gap between the query and document. \\
\\
Due to too much words added may bring in more noise, we want to fetch the most important words . So we take two steps to do this. Firstly, fetch words to be expanded. Secondly expand these candidate words. \\
\\
We use a medicine dictionary\footnote{http://www.cs.cmu.edu/\~lbing/data/emnlp-15-diel/emnlp-15-diel.tar.gz} to help 
measure the importance of the words.If the word in the dictionary, it is important, otherwise not. The medicine dictionary was extracted from \cite{bingimproving} who used freebase to extract entity instances of four types, namely disease, drug, ingredient, and symptom. After this, we get 3 more words per query(both on 2014 query and 2015 query), which is the centroid of our following work.

\subsection{Using WordNet and Google Knowledge Graph}
Query expansion with external common knowledge can surely shorten the distance between query and document. We explore two common knowledge, WordNet and Google Knowledge Graph. More Synonyms or more prior knowledge added can overcome polysemy and carry more information for query. \\

\begin{itemize}
\item {\bf WordNet}\footnote{https://wordnet.princeton.edu/} WordNet is a large lexical database of English developed by Princeton University. Synsets are interlinked by means of conceptual-semantic and lexical relations. We relate our expanded terms as entity to find the entity with the same semantic similarity.
\item {\bf Google Knowledge Graph}\footnote{https://developers.google.com/structured-data/customize/overview} Freebase\footnote{http://www.freebase.com} has been read-only since March 31st. After that it is making a commitment to Google Knowledge Graph. Thus we use Google Knowledge Graph to find more common knowledge to replace freebase, since its superior performance to query expansion\cite{xiong2015query}.
\end{itemize}


\subsection{Using Domain-related Knowledge}
On one hand, we can shorten our distance between the common knowledge(query) and the medical special knowledge(document) from the common knowledge, on the other hand, we can shorten it from the medical special knowledge. \\
\\
{\bf medicinenet}\footnote{http://www.medicinenet.com/} is committed to bring doctor's knowledge to {\bf You}. Next is what will find in medicinenet. Refer to www.medicinenet.com/\\
headache/symptoms.htm for more details.
\begin{quote}
Headaches can be associated with symptoms such as
\begin{itemize}
\item nausea
\item vomiting
\item pain in the eyes when looking into bright lights (photophobia)
\item dizziness
\item vertigo
\item tenderness of the scalp
\item tightness sensation in the head
\item stroke
\end{itemize}
\end{quote}
As showed, from medicinenet we can find similar symptoms for existing symptoms we observe from the query. We can infer the latent symptoms so that to improve the performance of the system. Afterwards, we crawled from medicinenet.com and extract the related symptoms appearing in our dict mentioned previously.\\
\\
On next section we will setup out experiment to check (i) whether this query expansion if efficient for the task and (ii) which method is the best compare others.


\section{Experiments}
\label{sec:expr}
In this section, we introduce our experimental methodology, including dataset, evaluation metrics and retrieval model.

\subsection{Datasets}

Both a document collection of biomedical texts and a set of topics were made available for participants of the track \cite{simpson2014overview}, The document collection consisted of articles in XML format using the National Library of Medicine¡¯s Journal Archiving and Interchange Tag Set. There were 733,328 articles in this NXML format, with each article uniquely identified by their PubMed Central Identifiers (PMCIDs). There were 30 topics in total, with 10 topics for each of the following three type: diagnosis, test, and treatment.\\
\\
The task at hand for each query was to return a set of biomedical texts which correspond to the patient¡¯s diagnosis, additional tests required, or the prescribed treatment, depending on the topic¡¯s type.\\
\\
We use 2014-TREC-CDS-track query as train data, while test on 2015-TREC-CDS-track.

\subsection{Evaluation Measurement}
In order to evaluate the performance of different query expansion, we adopted the official evaluation measure, i.e, MAP, P@10, NDCG@10, NDCG@20, NDCG in 2015-TREC-CDS-track.

\subsection{Experiments Setup}

We use SPUD language model as out base retrieval model. we use type, description and summary as our query words.

\section{Results}
\label{sec:results}
In this section, we assess the effectiveness of our proposed models for Information Retrieval on 2014-TREC-CDS-track data. In particular, we aim to answer the following research questions:
\begin{enumerate}
\item[Q1.] Whether this query expansion if efficient for the task?
\item[Q2.] Is the common knowledge more efficient or the domain-related knowledge?
\item[Q3.] What will happens if we combine the two strategy?
\end{enumerate}


\begin{table}[h]
\tiny
\begin{center}
\begin{tabular}{l|c|cccc|c}
\hline
\hline
\bf methods & \bf Query Length  & \bf MAP  & \bf P@10 & \bf NDCG@10 & \bf NDCG@20 & \bf NDCG \\
\hline
\hline
\bf baseline  & 101.36 & \bf0.1827 & \bf0.4200 & \bf0.4556 & \bf 0.4073 & \bf 0.4649 \\
\bf DICT      & 104.33 & 0.1718 & 0.3967 & 0.4279 & 0.3774 & 0.4521 \\
\bf DICTWN   & 127.97 & 0.1559 & 0.3533 & 0.3817 & 0.3476 & 0.4216 \\
\bf DICTMED  & 114.90 & 0.1683 & 0.3833 & 0.4061 & 0.3680 & 0.4473 \\
\bf DICTWNMED & 146.97 & 0.1393 & 0.3033 & 0.3238 & 0.2995 & 0.3829 \\
\hline
\hline
\end{tabular}
\end{center}
\caption{Result on 2014 query, {\bf DICT} represents words-expanded-query, {\bf DICTWN} is our WordNet Model, {\bf DICTMED} is our domain-related model, and {\bf DICTWNMED} combines WN and MED.}
\end{table}

Table 1 show the results we get on 2014 query. We can see all the runs failed compared with baseline. What is more, with more words expanded, the performed is even more worse. It seemed that too much noise bringed in it. We think SPUD is language model with document and topic most related, and the knowledge we use bring in other document distribution and topic distribution, which noise the original distribution.

\section{Conclusion}
\label{sec:conclusion}
%how do? achieve what? result analysis? conclusion? future work?
We proposed knowledge-based query expansion to Support clinical decisions. We expand query with common knowledge, ie., WordNet, Google Knowledge Graph and Domain-Related Knowledge ie., MedicineNet. We try to link the common knowledge we all know with domain-related knowledge since we search for clinical decisions which heavily rely on prior knowledge. Unfortunately, we can see all the runs failed compared with baseline. We think SPUD is language model with document and topic most related, the knowledge we use bring in other document distribution and topic distribution, which noise the original distribution. Our future work will focus on how to added the Knowledge Information but not noise original distribution.

\bibliography{Knowledge}
\end{document}

